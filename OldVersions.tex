Human learning is a vast and fascinating topic. We can adapt to novel situations and easily and quickly learn to how to use a new environment efficiently. 
One of the learning processes humans and other animals use can be categorized as reinforcement learning. %CW: RL is not really a process so much as a paradigm for studying human learning. You could say Pavlovian conditioning or associative learning are theories about learning processes but I think it would be better to stick with RL as a paradigm or framework
Reinforcement learning focuses on finding the action that maximizes a reward in specific situations. %CW: It's not strictly maximizing reward, since many situations it is not epistemologically possible to tell if a reward is being maximized or not. That's the whole point of the explore-exploit dilemma. Acquiring rewards would be better
This can be for example arriving at a new town and finding your new favourite restaurant, or preparing breakfast in a friends house. \\
So in its essence, it is mapping ones action to the situation in order to get the best possible outcome.\\ %Again, best possible outcome is not something we are usually able to verify
To fully understand the concept of human reinforcement learning, it is worth to take a look at the history of learning, or more precisely at animal learning, because from this, reinforcement learning in its computational form and human form drew its inspiration. %CW: I wouldn't point out animal learning here, since the experiment and models we use in this thesis are for human behavior. Just saying a look at the history of learning should be enough. It just so happens to be the case that the earliest studies were on animals
Therefore, this thesis will first give a quick overview of the history of animal learning, focusing on the most influential theories where computational reinforcement learning drew its inspiration from. Then, the computational ideas are explained, while those influence the study of human reinforcement learning and the other way around. One specific problem, the exploration and exploitation dilemma in particular will be discussed and its most commonly used study object the bandit problem. This will lead to the current research question of this thesis: How time pressure influences human exploration and exploitation strategies. \\ %CW: Come back to this paragraph once you've structured the rest of the intro. This paragraph will provide a map for the reader to guide them through the intro

The goal is to find a mathematical model, that will describe how humans decide under time pressure. %CW: the goal is not to only understand how humans decide under time pressure, but to use time pressure as a tool for peering into the processes people are using in reinforcement learning
Two exploration strategies dominate the current theories, the upper confidence bound and Thompson sampling (both will be discussed in detail in Chapter 2). %CW: Start with learning models first, and then move to sampling strategies
These two exploration strategies are testes against each other, because there one of them has higher computational costs, therefore, under time pressure, we assume that the "cheaper" strategy is chosen. Therefore, both strategies will be discussed in detail. 
Furthermore, learning models based on the rescorla wagner model are explained as well, since the bandit problem is not only based upon exploration, but also human learn from previous actions and the outcomes. So in order to create a good model for the situation, it is necessary to investigate both features and combine them. 
\\
Following the theoretical background, the experiment is presented with its behavior and model analysis. 

In the end, the results of both analysis are put into the current context (which is discussed in chapter 1, the introduction). 


\subsection{Proposal}






\todo[inline]{History of reinforcement learning and associative and non associative learning: starting with the bigger picture}
Imaging, you have just moved to a new city for a new job. You love your morning coffee and you want to get a nice cup every morning on your way to work. So what is the best way to get your coffee? How long do you search till you find a coffee shop to get your coffee every morning? \\ %CW: How long to search isn't the best way to frame the explore-exploit dilemma here. Rather, the choice between your usual cafe and a new cafe you haven't yet experienced
This and similar problems are the object of study in psychology and machine learning. These problems are referred to as reinforcement learning, up until recently researchers of both disciplines haven’t looked into the other discipline. %CW: Cite Sutton & Barto (1998). The second part of this claim isn't true. They originally published some key RL papers in psychology journals. And the origins or reinforcement learning are very deeply grounded in psychology. Thorndike's cats, Pavlov's dogs, to Clark Hull's theory of drives and B.F. Skinner's behavioralism, and then Rescorla-Wagner and TD learning being related to midbrain dopamine: these are the origins of reinforcement learning. Rather, you could say that current machine learning RL efforts have started to ignore psychology
However, recently Psychologists started to use algorithms that were built in the area of machine learning to model human behavior in problems as the one described above. These kinds of problems are called: exploration versus exploitation dilemmas. %Explore-exploit was coined during WWII so it's not new. 
Because in order to exploit your environment (e.g. to get the best coffee every morning), you have to explore your environment (e.g. find out which coffee shops makes good coffee). So the main question in those kinds of dilemmas is: when do humans stop exploring and start exploiting? 

%CW: I'm not sure that this framing of psychology vs. machine learning is useful here. Although you could make the argument about the descriptive problem (e.g., how people do something) vs. the prescriptive problem (e.g., how should you do something in an optimal way).
Psychologists and machine learning experts both try to solve this problem but from different points of view. While machine learning experts try to implement an algorithm that solves the problem, psychologists study how humans solve these problems with the help of experiments. These experiments measure behavior (e.g. which coffee shop is chosen, how fast was it chosen etc.) and sometimes even brain activity via EEG or fMRI. In the end, psychologists want to describe the process behind problem solving. This process is basically the same thing as an algorithm. Process and algorithm both describe how the problem is solved. Why study humans if you want to let a computer solve the same problems? Because humans can solve these problems very fast and near the optimal solution. So in the end, studying humans might help to develop algorithms that make computers solve these problems as fast and with a similar accurate or even better outcome as humans.

Our problem, the exploration versus exploitation dilemma, is also studied with experiments. The most used experiment is a two-armed bandit design. In here, participants are faced with two possible choices, each choice will give them a reward, one choice is (usually) better than the other one. Usually, participants are instructed to get the highest amount they can get within a specific amount of choices they can make.  This kind of experiment design is modeled after a one armed bandit machine, those where you pull the arm and get a reward (or maybe not). So each arm represents one choice, thus a two armed bandit would just be two one armed bandits next to each other. Therefore any amount of choices (e.g. arms) can be represented, this is then called a multi-armed bandit. The two most popular versions are the two-armed bandit and the four-armed bandit. 

In the field of reinforcement learning various strategies to solve the problem have been proposed.  One of them, \citep{gittins1979bandit}, found a computationally expensive algorithm that generated optimal solutions to our problem. However, it just took too long and took too many resources. %Not only too long but also requires an infinite horizon
This does not seem like humans use this as an underlying process, as we are still fast but sometimes are prone to a few errors. So another solution would be to look at heuristics, they serve to approximate the best solution. This just means, we won’t always get the best solution but are very near the best solution. There are two main algorithms that were extensively studied, Thompson sampling \citep{thompson1933likelihood} and the Upper Confidence Bound (UCB) \citep{auer2002finite}.
\todo[inline]{Insert a short paragraph about a neuroscience paper in there}

The Upper Confidence Bound \citep{auer2002finite} works in the way that it uses an uncertainty bonus \citep{srinivas2009gaussian}. This basically means that more uncertain choices get a bonus, thus their probability of being picked is higher. This corresponds to the idea, that you accumulate the most information in order to exploit these information later. In our coffee example, this would mean you would choose a coffee shop that you haven’t tried before. 

Thompson sampling on the other hand is more random oriented. Thompson sampling randomly draws a value function from the posterior, which is our knowledge about the bandit e.g. the arms and what rewards can be expected. From this random draw it greedily chooses an action, maximizing the immediate reward. In easier terms: we have no complete knowledge of the process of the machine (the bandit), so we randomly assign parameters to it, draw a value function, from which we want to choose an action that will maximize the immediate outcome.

Due to these two algorithms and experiments, a recent theory by \cite{gershman2018deconstructing}, proposes that humans use two different kinds of exploration strategies (e.g.  how to look for the best coffee shop), random exploration and directed exploration. These two concepts match the two algorithms we have met before. While Thompson sampling is an example of random exploration, UCB is an example of directed exploration. Furthermore, these concepts are driven by the role of uncertainty. Random exploration is influenced by absolute uncertainty, while directed exploration is influenced by relative uncertainty. 
\todo[inline]{go more into detail }

Going back to our quest to find the best coffee before work, there is another aspect which influences our decision making, time. Some mornings, time flies quickly and suddenly you are late for work, but you still want your coffee! How does this influence the exploring and exploiting strategy? More precisely how does time pressure influence absolute and relative uncertainty and therefore our exploring strategy? \\
One possibility would be that under time pressure a quicker greedier approach would be used. Therefore, rejecting the exploration bonus and randomly but upon previous knowledge choose a coffee shop. So instead of exploring a new coffee shop, you might just pick the one that has proven to be the best so far (or maybe choose another criteria). This would mean that directed exploration is influenced by time pressure and thus also relative uncertainty. What kind of influence has to be determined.\\
In order to investigate this, we conducted an experiment with a four armed bandit, which had half of its rounds under a time pressure. 


\newpage
\subsection{old Version}
Finding optimal solutions is computationally expensive \citep{gittins1979bandit}, however, simpler exploration strategies like heuristics can achieve approximately similar results with less cost. There are two major families for these heuristics: random and directed exploration.\\ 
Directed exploration is guided by uncertainty. Take the example of the coffee shop, you have some knowledge about the coffee from Starbucks, but next to the Starbucks is another coffee shop which you have no knowledge about. For directed exploration this uncertainty about the other coffee shops would mean, you will choose the coffee shop you know the least about in order to gain knowledge about it. In simpler words, the less you know about a choice, the more willing you are to explore it, therefore you explore the most uncertain option. This behavior is described as directed exploration.
Directed exploration is implemented by giving more uncertain options an exploration bonus, therefore making them more desirable to be chosen. A typical algorithm that uses directed exploration is the upper confidence bound (UCB) \citep{auer2002finite}. An action is chosen by maximizing the possible outcome of the posterior mean while adding the upper confidence bound, which works as an exploration bonus \citep{srinivas2009gaussian}. In simpler words: the action is chosen which will provide the highest outcome while simultaneously applying an exploration bonus in order to favour more uncertain options (thus gaining more information). \cite{gershman2018deconstructing} proposes that UCB is influenced by relative uncertainty, which is defined as \say{the difference in posterior uncertainty in between the two options} \citep{gershman2018uncertainty}.\\
Random exploration, on the other hand, uses uncertainty randomly. One of the oldest heuristics to solve the exploration exploitation dilemma in this way is Thompson sampling \citep{thompson1933likelihood}. Thompson sampling randomly draws a value function from the posterior. From this random draw it greedily chooses an action. Random exploration is sensitive to  absolute uncertainty \citep{gershman2018deconstructing}, which is defined as \say{the sum of posterior uncertainty across the options}\citep{gershman2018uncertainty}.\\
Now, imagine your coffee shop search before going to work. Your time to decide for a coffee shop is limited, you might be late for work already. The question now is, how is this time pressure affecting your decision? More precisely, how does time pressure affect uncertainty? Is time pressure leading to more absolute or relative uncertainty? 
\\
This thesis will focus on the influence of time pressure on absolute and relative uncertainty, investigating the relation between directed and random exploration. In order to do so, a four armed bandit with four different payoff conditions and two different time conditions is used. Payoff conditions are sampled from a normal distribution, the payoff conditions differ in their means and variances. Payoff conditions were designed so that absolute and relative uncertainty were represented by different arms.  
Through behavioral analysis we found a preference for higher variance when participants had no time pressure. During time pressure low variance options were favoured. A preference for relative uncertainty was found during unlimited time rounds. During time pressure however, there was a negative effect of relative uncertainty, expressing a negative exploration bonus, thus favouring the options with a low variance. 
In an upcoming model analysis, we will investigate how well different learning models ,Kalman Filter and Meantracker(a Bayesian version of the Rescorla-Wagner Model \citep{gershman2015unifying}), will behave with the two proposed exploration strategies (Thompson Sampling and UCB). Furthermore, linear ballistic model is modeled as well. 
