%\documentclass[main.tex]{subfiles}
%\begin{document}
\chapter{Introduction}
%This is better, but by framing it as having your parents visit, you transform the problem such that it wouldn't be safe to explore. (Rather wait until your parents, and then try out the unknown example). The goal in a bandit task is cumulative reward, so one trial with a bad outcome isn't so bad if you learn something valuable. You're making it sem as if it is the last trial, so it would only make sense to exploit! Also, why not choose 4 coffee shops since we use a 4-armed bandit? Not a big deal if you've already invested the time in the figures though.
Imagine you just moved to a new city for a new job, your parents plan a visit and want to go to a coffee shop with you. You have three coffee shops nearby (see Figure \ref{fig:Coffe_Example}). You have only tried one of them before and have absolutely no clue about the quality of the other two shops. Where will you go? Will you exploit your knowledge about the one coffee shop or will you discover one of the other coffee shops?

%You need to motivate why this coffee problem is important for cognitive science. What other problems are like it? Isn't there already a simple solution? Why should the reader care?
This is better known as the exploration-exploitation dilemma. How do you balance between \textit{exploiting} your current knowledge (e.g., going to the coffee shop you know) to acquire immediate rewards, compared to expanding your knowledge by \textit{exploring} (e.g., trying a new coffee shop), which can reduce the uncertainty about the environment. %CW: rewrote this to be more clear and so that it uses the same tense 
Gaining new information about the environment makes you better equipped to make future decisions (e.g., what coffee quality you get from which coffee shop). By expanding your knowledge you might even find coffee shops that offer better coffee than the one you already know.   %You're motivating the value of exploration, but maybe you haven't given enough motivation for exploitation


\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{Plots/CoffeExample.pdf}
    \vspace{-4cm}
    \caption[Coffe Shop Example]{How to choose a coffee shop. You are completely new in a city and you have multiple options, you do not know anything about the coffee from the different options.}
    \label{fig:Coffe_Example}
\end{figure}


%Why does it play a crucial role? Why is machine learning and AI interested in this coffee problem?
The exploration--exploitation dilemma plays a crucial role in the reinforcement learning framework \citep{sutton2018reinforcement}, which provides computational solutions to problems where learning occurs through interactions with an environment. %Introduce RL as abbreviation when describing reinforcement learning for the first time. Then use RL instead. You should reintroduce the abbreviation in each paragraph (the first time you use it)
Modern approaches to reinforcement learning used in machine learning and artificial intelligence are based on psychological models of associative learning \citep{sutton2018reinforcement}, which in turn can be traced back to early research on trial and error learning in animals. %citations for RL in machine learning and AI, citations for psychological models of associative learning (e.g, Bush and Mosteller, Rescorla and Wagner, Peter Dayan's paper where he first introduces the kalman filter for RL in humans), and citations for trial and error learning (Thorndike, Hull, Skinner)

 
This chapter is structured as follows. %I originally thought this was the structure for the whole thesis, so I added this clarification
To discover how the coffee problem and other instances of the dilemma can be solved, this thesis begins with a history of reinforcement learning, starting with early psychological theories of classical and operant conditioning. %Start with the broader theoretical question, and lean a little less on the coffee example here. It's ok to mention it, but you also need to connect it to the bigger picture of cognition--> why is it hard to behave intelligently when balancing between acquiring immediate rewards and acquiring information?
These theories provide the inspiration for models of associative learning, which are the computational origins of modern reinforcement learning. %I tried to rewrite this, since classical and operant condition are rather theories, not models. And the Rescorla-Wagner model is probably the first to be consider a model of associative learning and to be considered computationa. However, I think you should also clarify the distinction between RL and Human RL. RL is a field of AI (that only somewhat cares about human cognition), whereas Human RL is a subfield of psychology/neuroscience/cognitive science that uses RL models to understand cognition
In order to study how humans solve the exploration-exploitation dilemma, we use a multi-armed bandit problem. %Citation for bandit problem
By manipulating the available decision time in the task, we introduce computational limitations for the decision-maker in order to better understand the cognitive processes humans use to solve the exploration-exploitation dilemma. %CW: I added this sentence because I thought it was sorely needed. You don't mention the main innovation of this experiment. Rather, it sounds like just another vanilla bandit task
We first describe theoretically optimal solutions to outline computational approaches to the problem. However, these optimal solutions are largely intractable for real-world problems, which means they fail to provide a cognitively plausible theory for how humans tackle the problem. 
This is leads us to consider alternative strategies providing heuristic solutions to the exploration-exploitation dilemma. We treat these as candidate models of human behavior, with links to process level theories of human learning. %Cite Marr's levels. 
Last but not least, I present a brief description of the empirical framework used to study the exploration-exploitation dilemma and a introduction to the major theories that guide the trade-off between exploration and exploitation in humans. 
Finally, this chapter closes with an explanation of the main goals and hypothesis of this thesis. 

%CW: Here is where you should give a very short synopsis of what to expect in the introduction/thesis. I've rewritten substantial parts of this paragraph
Chapter \ref{ch:explo} of this thesis provides an overview of the reinforcement learning framework and the computational models we use to model human decision--makers. This includes an intuitive but sufficiently technical background for understanding the model comparison and analyses presented in later chapters. %This sentence sounds a bit familiar... maybe rephrase it :P
Chapter \ref{ch:experiment} describes the methods and design of the ``Speedy bandit'' experiment, which was used to study the influence of time pressure on human learning and choice behavior in a multi-armed bandit task. This chapter also provides an analysis of the behavioral results.
Chapter \ref{ch:results} presents analyses using computational models to predict choices and reaction times, in order to develop an understanding of the cognitive processes used by human decision--makers to navigate the exploration--exploitation dilemma. %TODO: revisit to add back in reaction time in case the LBA is included in the thesis
We compare different models and then examine how parameter estimates are affected by time pressure. %Added this sentence to describe what the model-based analysis add to the behavioral results 
Chapter \ref{ch:discussion} provides concluding remarks and a discussion of limitations and future directions. 



%The order of this section is a bit mixed up. Choose an order (thorndike vs. pavlov) and then stick to it. Also, the first paragraph in this section gives it all away in my opinion. 
\section[A history of reinforcement learning. It's raining cats and dogs]{A history of reinforcement learning\\ {\large It's raining cats and dogs}} 

Historically, the roots of reinforcement learning originated in studies of learning in cats \citep{thorndike1927law} and dogs \citep{pavlov1927conditional}.
%This section provides an introduction to reinforcement learning and its history in early research on human and animal learning. The roots of reinforcement learning can be traced back to Thorndike's (\citeyear{thorndike1927law}) law of effect and Pavlov's (\citeyear{pavlov1927conditional}) theory of classical conditioning, which were based on observations of learning in cats and dogs. %CW: rearranged the order to talk about cats and dogs 
In \citeyear{thorndike1927law}, Edward Thorndike investigated how cats learn through trial and error, and eventually formulating the law of effect.
The law of effect states that whenever an agent receives positive feedback, it is more likely to try the same action again. In contrast, after experiencing negative feedback, agents are more likely to avoid the action associated with it.
These dynamics are the basis for learning through interactions with an environment (e.g., reinforcement learning). Remember the coffee shop example: When you explore the environment and visit any given coffee shop, you receive feedback (i.e., quality of the coffee), which can influence future decisions. It seems plausible that the law of effects applies to this scenario, since if we experience a a good cup of coffee, we are more likely to go to the same coffee shop again. 
%What are the limitations of this theory? Why is this not a complete model of human RL? What is missing

%I was confused why this came here, whereas you have sections below with the reversed order. 
Not only cats but also dogs played a crucial part in foundational theories of learning through experience. 
Classical conditioning is an influential branch of psychology,  is that the association between a stimuli and reward can be learned, such that the stimuli predicts the reward. %Tried to make this clearer, but I'm still a bit lost in this paragraph about what the narrative is. %Provide citations for classical conditioning literature
%However, in this thesis we are even more interested in the computational models that emerged in order to describe classical conditioning. %Don't know what you are trying to say in this last sentence. You're already talking about classical conditioning. Why do you have to say you are interested in it?
From One of these is the Rescorla Wagner Model \citep{rescorla1972theory}. This model paved the way from passive associations to active learning based on a prediction error.
Ivan Pavlov accidentally discovered the learning concept of classical conditioning in 1927. Initially, he wanted to study the digestive system of dogs, but unintentionally, he observed that his dogs started to salivate when the caretaker arrived to give them food. Even before food was presented to the dogs, they started to respond in anticipation. This launched a series of experiments that would become the foundation for Pavlov's theory of classical conditioning \citep{pavlov1927conditional}.  


\subsection{Classical Conditioning}


%\subsubsection{Pavlov's Dogs}
%CW: Terminology overload. CS, CR, US, UR,... Maybe acronyms help, maybe they don't.
%Which previous observations?
In order to test his previous observation, Pavlov created an experiment where he introduced the sound of a metronome before the dogs received their food. %Citation missing
After a few repetitions, the dogs began to show the same reaction as in his earlier observation with the caretaker: they began to salivate in anticipation of the food.
Pavlov called this response a \emph{conditioned response}. A conditioned response is learned when an \emph{unconditioned stimulus (US)} (e.g., food) is paired with a neutral stimulus (e.g., the sound). Usually the neutral stimulus is presented before the US. After several repetitions of pairing these stimuli, the neutral stimuli becomes the \emph{conditioned stimuli (CS)}. The sound becomes associated with the food. After learning that the sound precedes the food, the dogs already begins to salivate upon hearing the sound, because they learned that hearing this sound predicts the arrival of food. %This paragraph was very clear

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{Plots/ClassicalConditioning.pdf}
    \caption{Classical Conditioning}
    \label{fig:Classical_Conditioning}
\end{figure}

%To clarify this, let us consider the dogs once more. The arrival of the food is the unconditioned stimulus. In order to eat it, they produce saliva, which is the unconditioned response. A conditioned stimulus is introduced before the arrival of the food, which is the sound of the metronome. After a few trials, the dogs learn that the sound anticipates the arriving of the food. Thus, the dogs can already prepare to act when the food arrives. So they already start to produce saliva when they hear the sound of the metronome. 

What have we learned from Pavlov's experiments and what does it have to do with human learning? Through the concept of classical conditioning, we see that it is possible to learn to predict certain events. The dogs learned that the arrival of the caretaker or the sound of the metronome preceded the arrival of food. This is one of the essential features of reinforcement learning: trying to predict future outcomes. %Very clear! Good job!

\subsection{Trial and Error Learning}
Unlike classical conditioning, trial and error learning is not focused on the learning to predict the future based on stimuli, but on learning behavior and the consequences of different actions. Learning occurs by identifying which actions lead to or are associated with good outcomes. This is the second main feature of reinforcement learning: association. 
Remember our coffee shop example: You are in a new environment with lots of different coffee shops to choose from and therefore with lots of possible actions. 
In order to choose actions which will lead to good outcomes, you need to try out different actions (e.g., visiting different coffee shops), which will most likely include errors. 
There is the possibility to try a new option and thus taking an action from which you learn more about your environment but also about your future decisions. %This sentence is a bit confusing. Maybe consider this alternative formulation? "However, trying out new actions can also lead to unexpectedly good outcomes, which can shape future behavior."

%REpeat content
Thorndike introduced the Law of Effect \citep{thorndike1927law}, which says that actions that are followed by satisfaction will occur again in the same situations, whereas other actions taken in the same situation that are not followed by a positive effect are not. %You said it better above
He also discovered that the stronger the satisfaction, the greater the action is associated with this kind of situation. Consequently, we see that the law of effect and reinforcement learning have the same essential principle: learning through interacting with the environment.

Thorndike studied this phenomenon using puzzleboxes \citep[pp.8-30]{thorndike1898animal}, which were a series locked boxes that could be opened by pulling various levers or buttons. %This explanation would have been useful earlier on. We've talked about thorndike for 3 or 4 paragraphs now, becore what he did finally emerges
In front of the box food was displayed, so that the cats inside were able to see the food. Cats were put into the box with no obvious way out. They had to find out which actions they had to perform in order to escape. They needed to pull or push specific button to get out.
The first time in the box, the cats scratched and tried many unsuccessful actions in order to escape. Eventually, the cats would happen upon the button or lever that would open the box and managed to escaped. During subsequent trials, the cat had already learned which actions helped it to escape the box, thereby reducing the number of unsuccessful actions that were attempted
% and promoting satisfying actions, which is called an associative linkage. %CW:I used parts of this commented out section above
The cats had learned to associate successful actions to the context (i.e., specific puzzle box), such that subsequent trials in the same context resulted in faster and faster escapes.

%You need a transition to behaviorism. This section is about cats and dogs, but skinner is more commonly associated with pigeons. also, the intro to this section (while far from perfect) only mentions thorndike and pavlov. So this comes as a bit of a surprise. How does Behavioraism fit into the bigger picture?
\subsubsection{Behaviorism}
%[Insert intro sentence to set this paragraph up]
\citet{skinner1963operant} introduced operant conditioning, the idea of reinforcing behavior through rewards or punishment. Unlike classical conditioning, it focuses on the action and not on the stimuli that is presented. In order to examine his theory he created the Skinner box, which differs from Thorndike's puzzle boxes in that the goal is not to escape but to obtain food rewards. %In a sense, this is sort if a combination of both Thorndike and Pavlov. Maybe this is a way you could transition to Behaviorism? In a Hegelian sense: Thesis (Pavlov) - Antithesis (Thorndike) --> Aufhebung (Skinner)
This allowed him to observe animals over a longer period of time and he could thanks to his observations indeed conclude that the rats that were put into this box learned favourable behavior through interacting with the environment. %Several issues with this sentence. Is length of observation the main advantage of Skinner? I think a different framing would be better. Animals, and then you transition to rats. How was he able to conclude that they learned favorable behavior? Isn't the correct terminology that they were conditioned to behave in such a way that produced rewards? 
This changed their behavior within the box entirely.  %Not sure how this follows logically 
Skinner continued to examine this phenomenon with various experiments and found he was able to shape the behavior of animals in specific directions. For example in a pigeon experiment, he was able to teach a pigeon to bowl a wooden ball. %Citation
He did this by reinforcing specific behaviors, which in the end, led to the pigeon learning to pick up the wooden ball and throw it in the right direction. Unlike Thorndike, Skinner concluded that the right actions are learned over the accumulation of random actions that will lead to the correct action. %How is this different from Thorndike? The argument here is unclear

%At the end of the discussion of each theory, provide the reader with some notions of the limitations and why the problem of learning is not yet solved. 


%This section is quite scattered. Start with a transition, then provide some intuitions first about what the RW model adds. Then go into the details. Rather than CS and US (which are rather confusing in my opinion), you could just provide the delta-rule prediction error equation. Then complete it by connecting it to the broader picture
\subsection{From Classical Conditioning to Prediction Error Learning}
%A transition is still missing. What does the RW model add that wasn't already understood? Previously, learning was understood as merely the passive coupling of experiences and rewards, whereas the RW model suggest learning involves making concrete predictions about reward, which are then updated based on the magnitude of the prediction error
In 1972, \citeauthor{rescorla1972theory} created a mathematical model to account for blocking in classical conditioning\footnote{The Rescorla-Wagner model was not the first model to use a prediction error learning rule, but it is the most famous one. The first model was created by Bush and Mosteller \citep{bush1951mathematical, bush1951model}}\citep{rescorla1972theory}. The Rescorla Wagner (RW) model is based on the assumption that the learning agent is faced with an unpredicted situation, e.g. when the predicted outcome is not the actual outcome. %The end of this sentence doesn't make sense

%TODO: provide equation
\begin{equation}

\end{equation}

%CW: I've commented out the following paragraph because it is too technical and offers little intuitions. It also ends up talking about blocking. While it was what they set out to understand, this isn't why the model is so famous and is not why you are interested in it for the thesis

%The focus is on the relationship between a conditioned stimulus and an unconditioned stimulus. 
%The learning of the CS is represented by the aggregate associative strength\footnote{A CS can consist of multiple component stimuli, each with an own associative strength, the combination of all those associative strength is the aggregated associative strength}, which adjust after each trial due to a predictive error. The aggregated associative strength says how reliable the CS predicts the US. 
%The prediction error is calculated by taking the difference between the asymptotic level of associative strength (or in easy: the point at which the US perfectly predicted) and the associative strength. If the prediction error is positive, the US can be learned and the associative strength is increasing after each trial until it is equal to the asymptotic level. Thus, if a new component stimuli is introduced, there is little to no learning effect, because the prediction error is already around zero. That is how the model accounts for blocking.

The RW model provided a new approach to learning, by recasting in a more active sense of forming concrete predictions of reward. The model requires the calculation of prediction error (i.e., the difference between expected and actual reward), which requires active predictions of future outcomes. In contrast, the process of learning in classical conditioning is merely a passive association of stimuli and reward. Thus, the RW model provided a new formal mechanism for how learning is not only passively learning associations between actions and outcomes, but rather involves active predictions of rewards. These predictions are then compared against the true outcome, and the magnitude of the error is used to improve future behavior. %I've rewritten these preceeding sentences to be more clear. Consider it as only a guideline
%From the idea of only associating rewards with specific actions to the idea of how trial and error learning is accomplished changed to the idea of predicting the future, observing the presence and updating ones own beliefs in order to minimize the difference between what one calculated and what actually happened in order to predict more accurately the next time and thus improve future performance. %CW: this seems like a rehash of what has already been said, but in less clear terms. 

This concept is similar to the delta-rule \citep{widrow1960adaptive}, a learning rule used in machine learning algorithms. %This is a nice connection, but this idea/section is not yet complete 
It is also called the least mean square method, because it looks for the weights that will minimize the square of all errors.  %CITATION missing


%You need a better stucture for this section. Also, I think you may have some misunderstandings regarding dynamic programming and MDPs. This is totally fine, since you are not expected to be an expert in every field. However, I would try to narrow the focus to what you are comfortable with in order to provide a better discussion. You don't have to cover everything! Just tell a clear and interesting story.
\subsection{A Computational Approach to Reinforcement Learning}
In the history of the reinforcement learning, two subsets emerged: one dealing with animal learning and the other one with dynamic programming. %What do you mean by subsets? Isn't all the sutton and barto work influenced by the animal learning literature? Also, be careful when talking about reinforcement learning, since as I mentioned above, there is somewhat of a difference between RL (in AL and ML), and RL used to understand human/animal behavior
Dynamic programming \citep{bellman1960dynamic} was developed by Robert Bellman in the 1950s and is concerned with optimal control \citep{sutton2018reinforcement}. 
Stochastic optimal control problems, and its solutions like Markov decision processes, are the problems that resemble the problems we considered the most. %Are you saying a MDP is a solution to a stochastic optimal control problem? This isn't true. An MDP is a way to describe the state space relationships by assuming the Markov principle (i.e., only the previous state is necessary for predicting the current state)
Markov decision processes are a formalization of sequential decision making \citep{sutton2018reinforcement}. This means they can be used to describe problems we have considered before.  
However, since dynamic programming deals with optimal solution, this as well as Markov decision processes, are computationally expensive and take a long time to be solved. We will consider a specific example of optimal control in the following section. 

\section{Human Reinforcement Learning}
Let us turn back to our coffee shop example. Imagine there are many coffee shops available in your neighborhood. Your goal is to accumulate the best coffee experience as you can. %This seems a bit silly here. Maybe there is a better way to frame it?
This might include some bad coffees but also exploring more shops to get an even better coffee experience. 
You have multiple options to choose from. You could either rely on your previous knowledge and get a coffee where you would know how it is, or you try something new, where you have no previous knowledge of how good it is.  %Exploration is not the opposite of relying on previous knowledge. Exploration often requires previous knowledge. Yet it's goal is to acquire more information
Sometimes, you might not even have some previous knowledge but still need to decide between multiple options. %In this case you behave randomly right? Isn't that all any of our models would predict? I wouldn't highlight this case
How is this done? How do you decide? 
\citeauthor{thompson1933likelihood} studied choice problems and introduced the experimental design of bandit problems in 1933 \citep{thompson1933likelihood}, which are named after the slot machines in casinos. Bandit problems were studied not only in the field of psychology but also in the field of artificial intelligence and machine learning. Each field of study influenced the other fields, creating an interdisciplinary topic.   

%I've taken the liberty here to rewrite this paragraph based on some commented out text below, which I thought provides a better introduction than what was currently here. It is far from perfect, but perhaps provides some inspiration for how to write this section
\subsection{Multi-armed Bandit Problem}
The multi-armed bandit (MAB) problem is inspired by slot machines in casinos (commonly called a one-armed bandit). Each slot machine can produce rewards when the arm is pulled, however the reward distribution is assumed to be unknown. The MAB problem, imagines if you were faced with a row of slot machines, each with different reward distributions, how would you decide which arm to pull? This setup provides a colorful metaphor for a choice problem, where outcomes have to be learned from experience and the goal is to acquire the highest total reward across multiple choices. This provides a very useful framework for studying the exploration-exploitation dilemma because....

 
%CW:This paragraph below didn't do a very good job of illustrating the task to someone unfamiliar with it, so I cut it for now. However, there are good parts that could be reused above
%Bandit problems are types of reinforcement learning problems with the exploration- exploitation dilemma as its core problem. It is an evaluative feedback problem, which means that a response is given about how good the chosen action was. The name and the problem were inspired by slot machines in casinos, which are called one armed bandits. When you pull the lever of such a machine it will give you a reward, if you are unlucky, you won't get any. The bandit problem in reinforcement learning is similarly structured, only that you always get a reward when you choose a lever. The levers are the given options. The most common type of bandit problem is the two armed bandit, which has two options. For example, you can imagine this as two slot machines next to each other or if we look at our coffee shop problem: you have two shops to choose from.
%There are multiple versions of this problem and are all summarized under the term multi-armed or \textit{K}-armed bandit, so you can basically have any number of options available. 

%Now what else is there to say about the MAB problem? Provide some examples of important findings, with citations. There are currently some bits that are repeats of the paragraph above, so please make sure it is updated
In a classical experiment using the MAB framework [CIATIONS], the decision--maker has a limited amount of decisions (i.e., limited horizon) and the main goal is to achieve the highest possible accumulated reward. %This is somewhat repeated above
The rewards are drawn from underlying probability functions, which are called reward functions. This just means that you do not only get the same reward from each option but there can be a variability in the rewards for each option. To make it more intuitive, think about the coffee example: you do not get the same quality of your coffee every time you buy one in the same shop; the quality can vary. Thus, our coffee example is an instance of a multi-armed bandit problem. But how can we solve it? 

%The name and problem itself was inspired by slot machines in casinos. Those machines produce unknown rewards if you pull their lever. In the bandit problem, each arm of a bandit is inspired by one of those slot machines. Thus, if there is more than one arm, you can imagine as if you would sit in front of more than one of those slot machines and now you are able to decide which lever to pull.
%In this problem, you are faced with choosing between different choices. After each choice, you get a numerical reward as a feedback. Usually, the amount of choices is limited and the main goal is to achieve the highest possible total reward. 
%The simplest version of the multi-armed bandit problem is the two-armed bandit. There only two choices are available. It is commonly used in economics where one of the arm usually gives a steady reward while the other one produces more uncertain rewards, which are drawn from a probability distribution, or reward functions. There are of course, variations where both rewards are drawn from different reward functions.  

%You could also give some intuitions about how the Gittins index works. It transforms the bandit task into a series of optimal stopping problems, thereby providing the right order in which to try out the options and when to stop trying them out any further
\paragraph{Gittins Index}
One approach to solving the MAB problem is through dynamical programming, which provides a framework for deriving an optimal solution. %CITE papers. When was dynamic programming first applied to RL or for  bandit problem?
\cite{gittins1979bandit} found an optimal algorithm for certain versions of the MAB problem, however it is not robust and cannot solve all of the variations of the problem. %Good. Maybe clarify more
The Gittins \citeyear{gittins1979bandit} index defines a solution for problems with a finite number of arms (i.e., options), with fixed reward probabilities, and for an \textit{infinite} learning horizon. This means that only in the limit of infinite time for learning and experimenting, does the Gittins index provide a truly optimal solution. %Focus on the infinite time assumption rather than the fixed reward distribution assumption (since we only use fixed rewards)
This is usually impossible to achieve in real life. %Usually? When is inifinite time possible?
%As \citet{cohen2007should} pointed out, real world environments change over time and so the fixed probabilities condition and the stable number of arms condition are both violated.  %Don't focus on changing rewards, since we only study fixed rewards
While optimal in theory, the Gittins index is impractical in real life. 
%This is not moving towards a different argument, of cognitive plausibility. The Gittins index is not truly optimal in finite time, however that alone doesn't necessarily suggest that Humans don't use a Gittins index stategy. Humans also aren't optimal. Rather, you should argue that the computational demands of dyanamic programming are unrealistic.
Humans are capable of solving these problems fairly well in real life. Thus we can assume that humans use another way to solve the problem than optimal solutions. But which?   

%Is this section not already a repeat of above? What is the main goal of this section?
\section{Exploration--Exploitation dilemma}
\cite{kaelbling1996reinforcement} describes the exploration--exploitation dilemma as one of the fundamental problems in reinforcement learning, which sets itself apart from other fields of machine learning. %How does it set itself apart?
The dilemma is concerned with finding the right trade-off between exploring to reduce uncertainty and exploiting by choosing the most rewarding optoin based on the current knowledge of the environment. Exploitation is helpful in order to gain immediate rewards, while exploration is helpful to inform future decisions, through learning which action produce good outcomes. 
 Accordingly, the bandit problem is an instance of the exploration-exploitation dilemma and thus also our coffee shop example. 

As proposed by \cite{cohen2007should} the trade-off is not only about one specific problem, but rather includes a spectrum of problems, backed up by \cite{mehlhorn2015unpacking}, they propose that there is not only one concept explaining it, but rather various. %Not sure what point you are trying to make by saying no only one concept explains it. Please clarfy this argument
Examples\footnote{\cite{hills2015exploration} offers a broad overview of areas which are concerned with the trade-off} of the exploration exploitation dilemma are: animal foraging \citep{cook2013exploration}, visual search \citep{chun1996just}, or risky decisions \citep{gonzalez2011instance, analytis2019make, schulz2018generalization}.

%Furthermore, \cite{cohen2007should} identified various factors that influence the decision on problems in this area, for example: the familiarity of the environment or how stationary the environment stays. %It's ok to illustrate the wide range of research in this area, but it's not clear where you are going with this

One possibility to look at this trade-off is by considering strategies how to explore an environment. In which ways can you look at the different coffee shops and reduce your uncertainty about the quality of the coffee? 

%I like this set up with heuristics, but I think you could do a better job of explaining heuristics. I would hope after having attended two summer institutes, you could provide a better account
\subsection{Exploration Strategies}
Full information models, like the Gittins index are computationally expensive and are not guaranteed to perform well in all circumstances. %"Full information models" is a new concept you haven't introduced before. What does it mean?
Another approach to solve or explore the environments are heuristics, which do not take all information into account \citep{parpart2018heuristics}, but perform reasonable well and can even outperform optimal solution strategies under realistic scenarios \citep{gigerenzer1996reasoning, gigerenzer1999simple, katsikopoulos2010robust}. As \cite{gigerenzer2009homo} pointed out, heuristics are solutions that perform well enough to a certain aspiration level. This means that there is a specific threshold to reach before accepting the solution. As we have seen with the Gittins index, optimal solution strategies are bound to very specific problems and usually need full information about the environment, these information also need to be stable, e.g. they are not allowed to change over time. Therefore, we can assume that most real-world problems are intractable for those kinds of algorithms. Heuristics which do not take all information into account are another approach to solve these problems. Two kinds of heuristics that solve the exploration-exploitation dilemma are widely discussed: random exploration and directed exploration.
%Be careful here. These are not heuristics that solve the exploration-exploitation dilemma. They are sampling strategies that COMBINED with a learning model can provide a solution to how to explore

\paragraph{Random Exploration}
\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{Plots/RandomExploration.pdf}
    \caption[Random Exploration]{Random Exploration. In purely random exploration strategies each option has the same probability to be chosen, e.g. an option can be drawn by throwing a dice. If we consider Thompson Sampling, the dice is weighted for the option that has the best outcome, e.g. to always get a six in a dice.}
    \label{fig:RandomExploration}
\end{figure}

%Epsilon-greedy is the purest form of random exploration, but it may be best described along a spectrum
Random exploration is tightly associated with $\epsilon$-greedy algorithms. \cite{wilson2014humans} suggest that they reflect simple heuristics and have less computational cost than directed exploration strategies.  
$\epsilon$-greedy strategies are those kinds of strategies that nearly always preform greedily with only a small probability, $\epsilon$, of sampling randomly \citep[pp.27-28]{sutton2018reinforcement}. A greedy action always chooses the immediate best reward, thus ignoring exploration. In order to adequately explore, the probability of making a completely random selection is included. 

%TODO: provide equation
\begin{equation}
P(c_i) = 
\end{equation}

%Maarten would say thompson sampling is a form of directed exploration, since it uses uncertainty information to guide exploration. Thompson sampling is stochastic, but it isn't really random. Rather, the softmax function with temperature might be a better example
Thompson \citeyear{thompson1933likelihood} sampling is another example of a random exploration strategy.
Thompson sampling is random oriented, which means it randomly draws a value function from the posterior, which is our knowledge about the bandit e.g. the arms and what rewards can be expected. %What does random oriented mean? 
From this random draw it greedily chooses an action, maximizing the immediate reward. In easier terms: we have no complete knowledge of the process of the machine (the bandit), so we randomly assign parameters to it, draw a value function, from which we want to choose an action that will maximize the immediate outcome.

%Don't think this statement is true at all
Random exploration is not as well researched as directed exploration, it is however, studied via model fitting.  
For example, \cite{daw2006cortical} studied a \textit{4}-armed bandit by comparing three different models. One of their models was a random exploration model, the $\epsilon$-greedy model, the other was a weighted belief updating model, which uses the soft-max rule\footnote{options are weighted due to their estimated values}. Their last model was an extended version of their second model with an exploration bonus attached (a directed exploration model). \citeauthor{daw2006cortical} found that the third model with the exploration bonus did not have any evidence from the experimental data. Comparing the remaining two models, the soft-max model had the best fit. %This paragraph is not well developed 

\paragraph{Directed Exploration}
\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{Plots/DirectedExploration.pdf}
    \caption[Directed Exploration.]{Directed Exploration. In directed exploration strategies option which we know less about are more likely to be chosen, in order to gain more information andthus reduce uncertainty about the environment.}
    \label{fig:DirectedExploration}
\end{figure}
Directed exploration strategies such as Upper Confidence Bound (UCB) sampling \citep{auer2002finite} are derived from optimal decision making strategies \citep{wilson2014humans}.  %How is UCB derived from optimal decision making strategies?
An exploration bonus is the main component of directed exploration behavior. The bonus leads to weighted exploration, this means that new options are not randomly selected but are encouraged by probabilities concerning their uncertainty. There has been evidence against the use of exploration bonuses in humans \citep{daw2006cortical}. However, more recently there has been more evidence found for the use of this strategy \citep{wu2017mapping, wu2018connecting, wu2018exploration, schulz2018searching, wilson2014humans, gershman2018deconstructing,knox2012nature, steyvers2009bayesian, }.    
\cite{wilson2014humans} assume that there are two main reasons why some studies did not find any exploration bonus. Firstly, direct exploration might be difficult to identify due to a confound between information and reward. They assume that the gained information is confounded with the reward the option yielded, which makes it difficult to identify directed exploration. 
Secondly, they assume that there is an information--ambiguity interaction. In other words: the more information you have about an option, the more it is ambiguous. This would mean, that participants that avoid ambiguity avoid information seeking and prefer exploiting their current knowledge.


Unlike Thompson sampling, UCB sampling \citep{auer2002finite} does not use immediate probabilities to decide which action to use, rather it works in the way that it uses an uncertainty bonus \citep{srinivas2009gaussian}. This basically means that more uncertain options (those you know less about) get a bonus, thus their probability of being picked is higher. This corresponds to the idea, that you accumulate the most information in order to exploit these information later.

%This section provides much needed citations of relevant literature, but it is not well organized. You always start off each paragrap with a \cite{}, but you should be oganizing your writing based on ideas, rather than on citations. Citations offer support and context for your ideas. You need to structure your writing such that you make an argument
\subsection{Current Theories}
The exploration--exploitation dilemma can be described with examples of choice problems, e.g., finding a new favourite coffee shop in a town you recently moved to and in a more scientific way with the bandit problem. %The coffee shop problem is a bandit problem. Not sure I understand your point here
How to solve these problems is not well understood. Optimal solutions have constraints that are nearly impossible to obtain. There are currently two main strategies upon which humans solve these problems, random and directed exploration.  

The first option is exploring an environment only by chance, which is called random exploration. Thompson sampling is an example of random exploration.
The second option is directed exploration, where sampling from more uncertain option is encouraged by adding an uncertainty bonus. The UCB is an instance of this exploration strategy.

\cite{wilson2014humans} investigated the effect of the horizon on those two strategies in order to understand how humans use these strategies or if they use them at all. Their carefully designed experiment allowed to distinguish between both kinds of strategies, due to the assumption that a varying horizon influences the way of information seeking and can even produce decision noise. Therefore, they were able to conclude that both strategies are indeed used and vary depending on the horizon. Given the opportunity to explore more e.g. having a higher horizon, participants rather used a directed exploration strategy, while under a smaller horizon random exploration was used.
In alignment to this finding \cite{gershman2018deconstructing} proposed that humans use a mixture of both exploration strategies, thus promoting a hybrid model of the UCB and Thompson sampling. He concluded that under relative uncertainty\footnote{the relative uncertainty between the option} there was a faster response time, which is consistent with the UCB. Furthermore, under absolute uncertainty\footnote{the overall uncertainty} there was a slower response time, which indicated the Thompson sampling \cite{gershman2018uncertainty}.

\cite{zajkowski2017causal} found a distinction in brain activity for both exploration strategies. In their trans magnetic stimulation experiment, using the same paradigm as \cite{wilson2014humans} found that when inhibiting the right frontopolar cortex, there was a negative influence on the use of directed exploration. It had, however, no effect on random exploration.

If we consider Marr's (\citeyear{marr1976understanding}) levels of explanation , we can argue that exploration strategies would fall into the first level: The computational level, which is supposed to tell us what a system is doing, in our case, which action is chosen. The next step would be to take a look at the second level: The process level. This deals with how one actually ends up making a decision and how the processes behind it work. In other words: How are the decisions made and which processes are guiding this decision?  %This is a very important paragraph but it has gotten buried

\cite{gottlieb2018towards} suggest that learning is governed by curiosity. They assume that animals and humans have intrinsic motivations that value specific types of information gain more than others, which are independent of gaining rewards. This kind of behavior can be studied under active sampling tasks, which are variations of the bandit problem. In order to actually see this phenomenon it is important that the agents is allowed to freely choose which stimuli (or button or source of information) it wants to explore\footnote{in plain information gain tasks, the same behavior can also be seen, suggesting there is an intrinsic motivation to gain information}, before it has to chose an action. 
Considering signal detection theory, or evidence accumulation theories\footnote{the agent has uncertainty about states in the environment and has some control over how much information it sample to reduce this uncertainty} like the drift diffusion model, \cite{gottlieb2018towards} notes that these theories assume that the agent has some knowledge about how to identify relevant cues, without taking into consideration how the agents determines which cue is relevant. 
Furthermore, curiosity is linked to memory \citep{gruber2014states, kang2009wick} and attention \citep{jepma2012neural}, which are important factors in learning.


\cite{collins2012much} studied the relation of working memory and reinforcement learning. By creating a task that allowed to separate working memory specific effects on learning, they were able to set up a new computational model, that was able to better estimate single reinforcement processes. This model is composed of two smaller models one model free reinforcement learning model, which represent slow and cumulative learning, and a second model which represent the working memory and is fast but with limited capacity. In a study in 2014, they were able to test their model on data gathered by schizophrenic patients \citep{collins2014working}, which are know to perform poorly in reinforcement learning tasks. Due to fitting the data to the model, they were able to discover that the impairment for reinforcement learning were governed by deficits in working memory that effects reinforcement learning. 


As we have seen in the passages above from experimental behavior, we can draw conclusion about the process level from Marr's level hypothesis, which is how is the process working. Thus the remaining question is, how and what process governs random and directed exploration. 

\section{Motivation and hypotheses}
Learning by interacting with the environment was first studied in animal learning research [cite thorndike, pavlov], and then developed into the field of reinforcement learning [cite Sutton]. Starting from theories of learning as a passive association of stimuli and reward [CITE], theories developed into more new conceptions requring actively making predictions about the world, and then updating based on the magnitude of the prediction error \citep{rescorla1972theory}. 

The exploration-exploitation dilemma is a central problem in the field of reinforcement learning, which focuses on the trade-off between exploring (reducing uncertainty about the environment) and exploiting (gaining rewards). In particular, the multi-armed bandit (MAB) framework is used to investigate this dilemma. Optimal strategies such as the Gittins Index are poorly suited in real world scenarios, because it assumes an infinite horizon and a stationary environment, both are criterion that are not in line with the real world, as there is a constantly changing environment and there can not be an infinite horizon. %I would remove the focus on stationary environment, since we also use a stationary environment. Rather, focus on the infinite horizon
However, heuristics can perform reasonable well and sometimes outperform optimal decision strategies. Heuristics can be used as exploration strategies in the bandit problem. %These heuristics are combined with psychologically inspired learning models
As we have seen, humans use two kinds of exploration strategies, random and directed exploration, in order to solve the exploration-exploitation dilemma tasks like the bandit problem. %You haven't shown that it is the case yet
These two heuristics can be formalized in the Thompson sampling for random exploration and the Upper Confidence Bound (UCB) for directed exploration. Both strategies are governed by uncertainty, but in different ways. Thompson sampling is governed by relative uncertainty while the UCB is governed by absolute uncertainty.

In order to understand the process managing these decision strategies, and thus gain insight into the cognitive processes underlying human learning, we applied time pressure onto a four armed bandit task. 
By manipulating the decision time, we induce computational limitations, which reduce the ability to sample from memory and accumulate evidence. This provides insight into the underlying processes that manages random and directed exploration, whether people use a combination of both or switch between them. 

\newpage




%\end{document}