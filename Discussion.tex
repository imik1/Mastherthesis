
\chapter{Discussion}\label{ch:discussion}
\paragraph{Summary of Behavior Data}
So far, we have seen that introducing a time limit on decision making in the bandit problem influences not only the reaction time but also the choice frequency, the entropy of the chosen option as well as the amounts of choosing the same option over and over again. 
In general, participants performed worse in limited time conditions than unlimited time conditions, however this was not significant. 
We also considered the choice frequency, which are the probabilities of each arm being chosen. We saw that participants chose the risk averse options in limited time conditions but the more risky, e.g the options with a higher variance, in the unlimited time conditions.
The Shannon entropy of each round gives us a choice distribution over the four options. This shows the average information gain for choosing an arm. Unlimited time rounds had a higher entropy than limited time rounds. This finding suggests that during unlimited time rounds, participants explored more. 
This can be confirmed by taking a look at the repetition of decisions, which is higher in limited time rounds. 

\paragraph{Summary of Modeling Results}
\paragraph{What we learn from the Results}
From the behavior results, we see a tendency to explore more during unlimited time rounds, while limited time rounds repeat their choice more often. In other words: during unlimited time rounds participants explore more, while under limited time rounds, participants exploit more.  
\paragraph{Set the Results & What we learn into context}
